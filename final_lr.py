# -*- coding: utf-8 -*-
"""Final_LR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DX3HzVmyLcZCIOoFpGAp4LNFF45WZpgv
"""
# November 2021 - CS 229
# Nicolas San Miguel
# Matheus Dias
# Lucas Sato

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn
from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix
import seaborn as sns
from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler
import math
from sklearn.neighbors import KNeighborsClassifier
import time
starttime = time.time()

def encode_and_normalize(app_df):
    # One Hot Encode
    data  = app_df
    to_encode = ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',
                'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS',
                'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE',
                'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE' ]

    encoded = pd.get_dummies(to_encode)
    data.drop(to_encode, axis = 1, inplace  = True)
    data.merge(encoded, left_index= True, right_index = True)

    # Inpute NaNs
    data.fillna(data.mean(), inplace= True)

    # Check multicollinearity - important otherwise L1 will choose randomly

    # NORMALIZE
    # robust scaler transform of the data ()
    # trans = MinMaxScaler()
    # data = trans.fit_transform(data)
    # data = pd.DataFrame(data)
    normalized_df=(data-data.min())/(data.max()-data.min())
    data = normalized_df

    outflag = 'Original Dataset'
    return data, outflag

def random_undersampling(data, desired_prevalence):
    """### Random Undersampling
    * Separates data where target = 1 and target = 0
    * keeps only a subset of samples where target = 0 such that the ratio between the two classes is 50-50
    """

    # separate all datapoints where target = 1, append them back onto the dataset multiple times
    zero_vals = data.loc[data['TARGET'] == 0]
    one_vals = data.loc[data['TARGET'] == 1]
    # data.append(one_vals)
    # data.append(one_vals)
    print('this is orig size',data.shape)
    print('this is zero vals size',zero_vals.shape)
    print('this is one vals size',one_vals.shape)

    # a = one_vals, b = zerovals + onevals  = total_vals
    # a/b = 0.08 current
    # a/(b-x) = 0.5 desired
    # a = 0.5b - 0.5x
    # a/0.5 = b-x
    # b  - a/.5  = x = number of samples to remove from b

    a = len(one_vals)
    b = len(zero_vals)
    # hardcoded the equation above,
    x = (b + a) - (a/desired_prevalence)
    tokeep = int(b - x)
    newzero_vals = zero_vals[:tokeep]

    print('this is new zero vals size',newzero_vals.shape)

    newdata = newzero_vals
    one_vals.TARGET.sum()

    newdata = newdata.append(one_vals)

    aug_data = newdata.append(one_vals) # aug data is more balanced - original data was unbalanced classifier
    newzero_vals.TARGET.sum()
    one_vals.TARGET.sum()
    aug_data.TARGET.sum()
    one_vals.shape

    outflag = 'Randomly Undersampled Dataset'
    return aug_data, outflag

def random_oversampling(data, desired_prevalence):
    """### Random Oversampling
    * Separates data where target = 1 and target = 0
    * Duplicates samples where target = 1 until the ratio between the two classes is 50-50
    * Randomly draws from this new dataset to run analyses
    """

    # separate all datapoints where target = 1, append them back onto the dataset multiple times
    zero_vals = data.loc[data['TARGET'] == 0]
    one_vals = data.loc[data['TARGET'] == 1]

    # hardcoded the equation above,
    a = len(one_vals)
    b = len(zero_vals)
    newa  = b*desired_prevalence / (1-desired_prevalence)

    duplicates2make = math.floor(newa/a) # 0 if a already 50%, 1 id a is 25%, etc.
    #^^floor function is arbitrary, could also be ceil

    newdata = one_vals
    for i in range(duplicates2make):
      newdata = newdata.append(one_vals)

    print('this is new data shape',newdata.shape)
    aug_data = data.append(newdata)
    outflag = 'Randomly Overampled Dataset'
    return aug_data, outflag

def smote(data,desired_prevalence):
    """### SMOTE
    * Separates data where target = 1 and target = 0
    * keeps only a subset of samples where target = 0 such that the ratio between the two classes is 50-50
    * without replacement!!
    """

    # separate all datapoints where target = 1, 0; start a df of synthetic points with the same columns as orig data
    zero_vals = data.loc[data['TARGET'] == 0]
    one_vals = data.loc[data['TARGET'] == 1]
    synth_data = pd.DataFrame(columns=data.columns)
    synth_data = synth_data.drop('TARGET', axis = 1)

    # # # # # # # # # Synthetic Minority Oversampling TEchnique (SMOTE)
    # number of synth examples to make for desired prevalence
    synth2add = (desired_prevalence*len(zero_vals) + len(one_vals)*(desired_prevalence-1))/(1-desired_prevalence)

    # we will use k-nearest neighbors later so this sets that up
    X_train, X_test, Y_train, Y_test = train_test_split(one_vals.drop('TARGET', axis = 1), one_vals.TARGET, test_size=0.20)
    classifier = KNeighborsClassifier(n_neighbors=5)
    classifier.fit(X_train, Y_train)

    for i in range(int(synth2add)):
        # randomly sample a single value from the minority dataset
        rand_sample = one_vals.sample()
        rand_sample = rand_sample.drop('TARGET', axis = 1)

        # find the 5 nearest neighbors to that randomly sampled datapoint
        neigh_dist, neigh_ind = classifier.kneighbors(rand_sample, n_neighbors=5, return_distance=True)

        # randomly choose a new datapoint from the k-nearest neighbors
        neigh_ind = neigh_ind.flatten()
        idx2interp = np.random.choice(neigh_ind)
        rand_sample2 = one_vals.iloc[idx2interp]

        # create a new, synthetic, datapoint between the original sample and the
        # <<interpolate between rand_sample and rand_sample2>>
        rand_sample = rand_sample.squeeze() # conver rand_sample from dataframe to series
        synth = np.sqrt([(a-b)*(a-b) for a, b in zip(rand_sample, rand_sample2)])
        synth = pd.DataFrame(synth.reshape((1, 105)), columns=list(synth_data.columns))

        synth_data = synth_data.append(synth)
        if i % 1000 == 0:
            print('this is i:',i)
    one_vals.drop('TARGET',axis=1)
    newones = one_vals.append(synth_data)
    # print(len(synth_data))
    # print(len(one_vals))
    # print(len(newones))
    newones['TARGET'] = 1
    aug_data = zero_vals.append(newones, ignore_index=True)
    outflag = 'SMOTE Dataset'
    return aug_data, outflag

def give_info_on_aug_data(aug_data):
    """### Info on the augmented dataset
    * Size of zeros, ones, total
    * Prevalence

    -- prints  info about the shape of the aug_data df.
    returns nothing
    """

    zero_vals = aug_data.loc[aug_data['TARGET'] == 0]
    one_vals = aug_data.loc[aug_data['TARGET'] == 1]
    # zero_vals = aug_data.loc[data['TARGET'] == 0]
    # one_vals = aug_data.loc[data['TARGET'] == 1]
    print('zero vals len',len(zero_vals))
    print('one vals len',len(one_vals))
    print("prevalence:",len(one_vals)/len(aug_data))
    print('the total size is :',len(aug_data))

def preprocessing(data,aug_data):
    """# Preprocessing"""

    # Split dataset
    # get the test data from the original data
    _, x_test, _, y_test = train_test_split(data.drop('TARGET', axis = 1), data.TARGET, test_size=0.25, random_state=None)
    # get the train data from the augmented dataset
    x_train, _, y_train, _ = train_test_split(aug_data.drop('TARGET', axis = 1), aug_data.TARGET, test_size=0.25, random_state=None)

    print(x_train.shape)
    print(y_train.shape)

    aug_data.shape
    return x_train, y_train, x_test, y_test

def define_models(x_train, y_train):
    """# Define Models"""
    # Logistic Regression with Lasso regularization
    model = LogisticRegression(penalty='l1', solver='liblinear') #check liblinear

    scores = cross_val_score(model, x_train[:10000], y_train[:10000], cv=3, scoring = 'roc_auc')
    model.fit(x_train[:10000], y_train[:10000])
    scores.mean()
    return model, scores

def grid_search(x_train,y_train):
    # reference website: https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/
    """# Optimize Hyperparameters"""
    # model = LogisticRegression(x_train, y_train)
    model = LogisticRegression()

    solvers = ['liblinear']
    penalty = ['l1']
    c_values = [100, 10, 1.0, 0.1, 0.01]

    # define grid search
    grid = dict(solver=solvers,penalty=penalty,C=c_values)
    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='roc_auc',error_score=0)
    grid_result = grid_search.fit(x_train[:10000], y_train[:10000])

    # summarize results
    print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
    means = grid_result.cv_results_['mean_test_score']
    stds = grid_result.cv_results_['std_test_score']
    params = grid_result.cv_results_['params']
    for mean, stdev, param in zip(means, stds, params):
        print("%f (%f) with: %r" % (mean, stdev, param))
    return grid_result, cv, means, stds, params

def no_grid_search():
    # reference website: https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/
    solvers = ['liblinear']
    penalty = ['l1']
    c_values = [0.1]
    # define grid search
    grid = dict(solver=solvers,penalty=penalty,C=c_values)
    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
    # print('this is cv:',cv)
    return cv

def xval(x_train, y_train, cv):
    results_xgb =  cross_val_score(model, x_train.iloc[:20000,:], y_train[:20000], cv=cv, scoring = 'accuracy')
    results_xgb.mean()
    return results_xgb

def plotting(model, x_train, y_train, x_test, y_test):
    """# Plotting"""

    # importance = model.coef_[0]
    # plt.plot(c_values,means)

    # plt.xlabel('Inverse of Regularization Strength, c')
    # plt.ylabel('Scoring Value');

    # plt.show()

    # plt.plot(c_values,stds)
    # plt.show()

    # confusion matrix
    model.fit(x_train.iloc[:20000,:], y_train[:20000])
    y_hat = model.predict(x_test)
    # cf_matrix = confusion_matrix(y_hat,y_test)
    # ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')
    # ax.set_title('Random Forest')
    # ax.set_ylabel('\nPredicted Values')
    # ax.set_xlabel('Actual Values ');

    # Add title here
    # importance = model.coef_[0]
    # plt.bar([x for x in range(len(importance))], importance)
    # plt.show()

    # Feature Selection - Only variables that have abs(weight) larger than 0.2
    coefficients = pd.DataFrame(zip(x_train.columns, np.abs(model.coef_[0])), columns = ['variable', 'coeff'])
    selected_features = list(coefficients.sort_values('coeff', ascending= False).head(10).variable)
    coefficients.sort_values('coeff', ascending= False).head(10)

    print('these are the coefficients:\n',coefficients)
    print('these are the selected features:\n',selected_features)
    return coefficients, selected_features, y_hat

def logreg_no_reg(model, x_train, y_train):
    # Logistic Regression with no regularization
    model2 = LogisticRegression()
    model2.fit(x_train[selected_features], y_train)
    scores2 = cross_val_score(model, x_train, y_train, cv=3, scoring = 'roc_auc')
    scores2.mean()
    return model2, scores2

def plotting_auc(model2, selected_features, x_test, y_test):
    # Plot ROC-AUC curve
    y_pred_proba = model2.predict_proba(x_test[selected_features])[::,1]
    fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)

    #create ROC curve
    # plt.plot(fpr,tpr)
    # plt.ylabel('True Positive Rate')
    # plt.xlabel('False Positive Rate')
    # plt.show()
    return y_pred_proba

def print_stats(aug_data, outflag, results_xgb, scores2, y_test, y_pred_proba, y_hat):
    # get one_vals
    one_vals = aug_data.loc[aug_data['TARGET'] == 1]

    # stats from throughout
    print("Balancing method used:", outflag)
    print("prevalence:", len(one_vals)/len(aug_data))
    print("xgb mean:",results_xgb.mean())
    print("scores2 mean:",scores2.mean())

    # AUC Score
    print("AUC score:", roc_auc_score(y_test,y_pred_proba))

    # Model Accuracy: how often is the classifier correct?
    print("Accuracy:", metrics.accuracy_score(y_test, y_hat))

    # Model Precision: what percentage of positive tuples are labeled as such?
    print("Precision:", metrics.precision_score(y_test, y_hat))

    # Model Recall: what percentage of positive tuples are labelled as such?
    print("Recall:", metrics.recall_score(y_test, y_hat))

def run_against_training_data(x_train,y_train):
    # !!!!!!!!!!!!!! this is not good for statistical use !!!!!!!!!!!!!!!!
    # this fits the model to the training data and caalculates its metrics
    # they are all favorable as expected since it was the data that was used to train the model
    # in the first place -  this is just to show that it is working and our problems
    # are coming from our process :/

    model.fit(x_train.iloc[:100000,:], y_train[:100000])
    y_hat = model.predict(x_train)
    print('!!!!!!! this is not good for statistical use !!!!!!')
    # Model Accuracy: how often is the classifier correct?
    print("Accuracy:",metrics.accuracy_score(y_train, y_hat))
    # Model Precision: what percentage of positive tuples are labeled as such?
    print("Precision:",metrics.precision_score(y_train, y_hat))
    # Model Recall: what percentage of positive tuples are labelled as such?
    print("Recall:",metrics.recall_score(y_train, y_hat))



#######################################################################################################################
##################################################### main ############################################################
#######################################################################################################################
pd.set_option("display.max_columns", None)
app_df = pd.read_csv("application_data.csv") # I added this bc the cell above wouldn't import data for me - Nico

data, outflag = encode_and_normalize(app_df)

print('this is the time before balancing:', time.time() - starttime)
###### balance the dataset - run one of these three methods
# aug_data, outflag = random_undersampling(data, desired_prevalence = 0.3)
aug_data, outflag = random_oversampling(data, desired_prevalence = 0.35)
# aug_data, outflag = smote(data, desired_prevalence = 0.3)
print('this is the time after balancing:', time.time() - starttime)

# give_info_on_aug_data(aug_data) # gives info on the augmented dataset
x_train, y_train, x_test, y_test = preprocessing(data,aug_data) # separates data and
print('this is the time after preprocessing:', time.time() - starttime)

model, scores = define_models(x_train, y_train) # sets up the first model
print('this is the time after defining models:', time.time() - starttime)

# grid_result, cv, means, stds, params = grid_search(x_train,y_train) # this runs a grid search over
cv = no_grid_search() # this skips the grid search
print('this is the time after grid search or not:', time.time() - starttime)

results_xgb = xval(x_train,y_train,cv)
coefficients, selected_features, y_hat = plotting(model, x_train, y_train, x_test, y_test) #
model2, scores2 = logreg_no_reg(model, x_train, y_train) # sets up logreg without the l1 regularization
print('this is the time after the first plots:', time.time() - starttime)

y_pred_proba = plotting_auc(model2, selected_features, x_test, y_test) # plots the auc curve
print_stats(aug_data, outflag, results_xgb, scores2, y_test, y_pred_proba, y_hat) # prints metrics and info
# run_against_training_data(x_train, y_train) # to run against training data - this should be high and is not good for drawing conclusions
print('this is the total runtime:', time.time() - starttime)